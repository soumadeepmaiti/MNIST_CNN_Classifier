# Model_Results_and_Visualizations.ipynb

# Import necessary libraries
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

# 1. Load the Trained Model
model = CNN()
model.load_state_dict(torch.load('saved_models/mnist_cnn_final.pth'))
model.eval()

# 2. Generate Predictions and Confusion Matrix
all_labels = []
all_preds = []
for images, labels in test_loader:
    outputs = model(images)
    _, preds = torch.max(outputs, 1)
    all_labels.extend(labels.numpy())
    all_preds.extend(preds.numpy())

conf_matrix = confusion_matrix(all_labels, all_preds)

# 3. Plot Confusion Matrix
plt.figure(figsize=(8, 8))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
plt.xticks(np.arange(10), [str(i) for i in range(10)])
plt.yticks(np.arange(10), [str(i) for i in range(10)])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

for i in range(10):
    for j in range(10):
        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment="center",
                 color="white" if conf_matrix[i, j] > conf_matrix.max() / 2 else "black")
plt.tight_layout()
plt.show()

# 4. Calculate Precision, Recall, and F1-Score
precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)

# 5. Plot Precision, Recall, and F1-Score
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
metrics = [precision, recall, f1_score]
titles = ['Precision', 'Recall', 'F1-Score']

for i, metric in enumerate(metrics):
    axes[i].bar(np.arange(10), metric)
    axes[i].set_title(titles[i])
    axes[i].set_xlabel('Class')
    axes[i].set_ylabel(titles[i])
    axes[i].set_ylim([0, 1])

plt.tight_layout()
plt.show()

