# EDA_MNIST.ipynb

# Import necessary libraries
import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# 1. Loading the Dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)

# 2. Visualizing Sample Images
dataiter = iter(train_loader)
images, labels = dataiter.next()
fig, axes = plt.subplots(figsize=(10, 10), ncols=8)
for i in range(8):
    ax = axes[i]
    ax.imshow(images[i].numpy().squeeze(), cmap='gray')
    ax.set_title(f'Label: {labels[i].item()}')
    ax.axis('off')
plt.show()

# 3. Data Distribution Analysis
class_counts = np.bincount(train_dataset.targets.numpy())
class_labels = np.arange(10)
plt.figure(figsize=(10, 6))
plt.bar(class_labels, class_counts, tick_label=class_labels)
plt.xlabel('Digit Label')
plt.ylabel('Frequency')
plt.title('Distribution of Digit Classes in MNIST Training Set')
plt.show()

# 4. Preprocessing (if any additional steps are required)
# Typically normalization is sufficient for MNIST, but this is where you would add more.

